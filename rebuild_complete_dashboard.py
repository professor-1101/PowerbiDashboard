#!/usr/bin/env python3
"""
Rebuild BugTracking_Complete.xlsx with new CSV data
Properly mapped to 74-field structure
"""

import pandas as pd
import numpy as np
from openpyxl import load_workbook
from openpyxl.styles import PatternFill, Font, Alignment
from datetime import datetime, timedelta
import re

print("=" * 80)
print("Ø³Ø§Ø®Øª Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ú©Ø§Ù…Ù„ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯")
print("=" * 80)

# ============================================================================
# STEP 1: Load CSV data
# ============================================================================

print("\nğŸ“¥ STEP 1: Ø®ÙˆØ§Ù†Ø¯Ù† CSV...")
csv_file = "Untitled query (1).csv"
df_csv = pd.read_csv(csv_file, encoding='utf-8-sig')
print(f"   âœ… {len(df_csv)} Ø¨Ø§Ú¯ Ø§Ø² CSV Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯")

# ============================================================================
# STEP 2: Load original Excel structure
# ============================================================================

print("\nğŸ“‚ STEP 2: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø§ØµÙ„ÛŒ Excel...")
wb = load_workbook('BugTracking_Complete.xlsx')
ws_raw = wb['raw_data']

# Get original headers
original_headers = []
for col in range(1, 75):  # 74 fields
    header = ws_raw.cell(1, col).value
    original_headers.append(header)

print(f"   âœ… Ø³Ø§Ø®ØªØ§Ø± Ø§ØµÙ„ÛŒ: 74 ÙÛŒÙ„Ø¯")

# Clear all data rows (keep header)
ws_raw.delete_rows(2, ws_raw.max_row)
print(f"   âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ù¾Ø§Ú© Ø´Ø¯")

# ============================================================================
# STEP 3: Helper functions
# ============================================================================

def clean_severity(value):
    if pd.isna(value):
        return "Medium"
    severity_map = {
        "1 - Critical": "Critical",
        "2 - High": "High",
        "3 - Medium": "Medium",
        "4 - Low": "Low"
    }
    return severity_map.get(str(value).strip(), str(value))

def extract_category(value):
    if pd.isna(value) or value == "":
        return "Other"
    match = re.match(r'^([A-Z]+)', str(value))
    return match.group(1) if match else str(value)

def extract_name_and_id(value):
    if pd.isna(value) or value == "":
        return "", ""
    match = re.match(r"(.+?)\\s*<RPK\\\\(.+?)>", str(value))
    if match:
        return match.group(1).strip(), match.group(2).strip()
    return str(value), ""

def extract_project_sprint(value):
    if pd.isna(value) or value == "":
        return "", ""
    parts = str(value).split("\\\\")
    project = parts[0] if len(parts) > 0 else ""
    sprint = parts[1] if len(parts) > 1 else ""
    return project, sprint

def parse_datetime(value):
    if pd.isna(value) or value == "":
        return None
    try:
        return pd.to_datetime(value, format='%m/%d/%Y %I:%M:%S %p')
    except:
        try:
            return pd.to_datetime(value)
        except:
            return None

# ============================================================================
# STEP 4: Map CSV data to 74-field structure
# ============================================================================

print("\nğŸ”„ STEP 3: Ù†Ú¯Ø§Ø´Øª Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ 74 ÙÛŒÙ„Ø¯...")

# Create DataFrame with all 74 fields
df_mapped = pd.DataFrame()

# Process each CSV row
for idx, csv_row in df_csv.iterrows():
    row_data = {}
    
    # 1-8: Core fields
    row_data['BugID'] = int(csv_row['ID']) if pd.notna(csv_row['ID']) else 0
    row_data['Title'] = csv_row['Title'] if pd.notna(csv_row['Title']) else ""
    row_data['Description'] = csv_row['Description'] if pd.notna(csv_row['Description']) else ""
    row_data['Severity'] = clean_severity(csv_row['Severity'])
    row_data['Priority'] = int(csv_row['Priority']) if pd.notna(csv_row['Priority']) else 2
    row_data['State'] = csv_row['State'] if pd.notna(csv_row['State']) else "Open"
    row_data['Category'] = extract_category(csv_row.get('Bug Type', ''))
    row_data['IsRegression'] = 1 if pd.notna(csv_row.get('Tags', '')) and 'regression' in str(csv_row.get('Tags', '')).lower() else 0
    
    # 9: is_escaped (MOCK - set to 0)
    row_data['is_escaped'] = 0
    
    # 10-18: Project/Team/Module
    row_data['ProjectID'] = ""  # Not in CSV
    row_data['ProjectName'] = csv_row.get('Team Project', '')
    row_data['TeamName'] = csv_row.get('Team Project', '')
    row_data['TeamID'] = ""  # Not in CSV
    row_data['ModuleID'] = ""  # Not in CSV
    row_data['ModuleName'] = ""  # MOCK - for manual entry
    project, sprint = extract_project_sprint(csv_row.get('Iteration Path', ''))
    row_data['SprintName'] = sprint
    row_data['SprintID'] = ""  # Not in CSV
    row_data['Tags'] = csv_row.get('Tags', '')
    
    # 19-20: External
    row_data['ExternalTicketID'] = ""  # Not in CSV
    row_data['Comments'] = int(csv_row.get('Comment Count', 0)) if pd.notna(csv_row.get('Comment Count')) else 0
    
    # 21-28: People
    row_data['ReporterName'] = ""  # Not in CSV
    row_data['ReporterID'] = ""  # Not in CSV
    
    assignee_name, assignee_id = extract_name_and_id(csv_row.get('Assigned To', ''))
    row_data['AssigneeName'] = assignee_name
    row_data['AssigneeID'] = assignee_id
    
    row_data['VerifierName'] = ""  # Not in CSV
    row_data['VerifierID'] = ""  # Not in CSV
    
    resolver_name, resolver_id = extract_name_and_id(csv_row.get('Closed By', ''))
    row_data['ResolverName'] = resolver_name
    row_data['ResolverID'] = resolver_id
    
    # 29-40: Dates (MOCK for most)
    row_data['CreatedDate'] = parse_datetime(csv_row.get('State Change Date'))  # Approximation
    row_data['TriageDate'] = None  # MOCK
    row_data['AssignedDate'] = parse_datetime(csv_row.get('State Change Date'))  # Approximation
    row_data['StartedDate'] = None  # MOCK
    row_data['InProgressDate'] = None  # MOCK
    row_data['ReadyForRetestDate'] = None  # MOCK
    row_data['ResolvedDate'] = parse_datetime(csv_row.get('Resolved Date'))
    row_data['VerifiedDate'] = None  # MOCK
    row_data['DoneDate'] = None  # MOCK
    row_data['ClosedDate'] = parse_datetime(csv_row.get('Closed Date'))
    row_data['LastModifiedDate'] = parse_datetime(csv_row.get('State Change Date'))
    row_data['DueDate'] = parse_datetime(csv_row.get('Target Date', csv_row.get('Due Date')))
    
    # 41-53: Workflow metrics (MOCK)
    row_data['ReopenCount'] = 0  # MOCK
    row_data['FirstReopenDate'] = None  # MOCK
    row_data['LastReopenDate'] = None  # MOCK
    row_data['PreviousState'] = ""  # MOCK
    row_data['StateTransitionCount'] = 1  # MOCK - minimum
    row_data['TriageDurationHrs'] = 0  # MOCK
    row_data['ActiveDurationHrs'] = 0  # MOCK
    row_data['InProgressDurationHrs'] = 0  # MOCK
    row_data['ReadyForRetestDurationHrs'] = 0  # MOCK
    row_data['StateHistory'] = ""  # MOCK
    row_data['AssigneeChangeCount'] = 0  # MOCK
    row_data['StateChangeCount'] = 1  # MOCK
    row_data['FixAttempts'] = 1  # MOCK
    
    # 54-60: Effort (MOCK - for manual entry)
    row_data['AnalysisEffortHrs'] = None
    row_data['DevEffortHrs'] = None
    row_data['FixEffortHrs'] = None
    row_data['TestEffortHrs'] = None
    row_data['ReopenEffortHrs'] = None
    row_data['TotalEffortHrs'] = None
    row_data['EstimatedEffortHrs'] = None
    
    # 61-66: Time metrics
    created_date = row_data['CreatedDate']
    closed_date = row_data['ClosedDate']
    
    if pd.notna(closed_date) and pd.notna(created_date):
        delta = (closed_date - created_date).total_seconds() / 3600
        row_data['LeadTimeHrs'] = round(delta, 2) if delta >= 0 else 0
        row_data['CycleTimeHrs'] = round(delta * 0.7, 2)  # Approximation
    else:
        row_data['LeadTimeHrs'] = None
        row_data['CycleTimeHrs'] = None
    
    row_data['ResponseTimeHrs'] = 24  # MOCK - default 1 day
    row_data['WaitTimeHrs'] = 0  # MOCK
    row_data['ActiveWorkTimeHrs'] = row_data['LeadTimeHrs']  # Approximation
    
    # Age for open bugs
    if row_data['State'] in ['Open', 'Active', 'In Progress', 'triage', 'Waiting']:
        if pd.notna(created_date):
            age_delta = (datetime.now() - created_date).days
            row_data['AgeDays'] = age_delta if age_delta >= 0 else 0
        else:
            row_data['AgeDays'] = 0
    else:
        row_data['AgeDays'] = 0
    
    # 67-70: Resolution
    row_data['RootCause'] = ""  # MOCK - for manual entry
    row_data['Resolution'] = ""  # MOCK - for manual entry
    row_data['CloseReason'] = csv_row.get('Closed Reason', '')
    row_data['TestCaseID'] = ""  # MOCK - for manual entry
    
    # 71-74: Quality metrics
    is_duplicate = 1 if str(row_data['CloseReason']).strip() == 'Duplicate' else 0
    row_data['IsDuplicate'] = is_duplicate
    row_data['DuplicateOfBugID'] = ""  # MOCK - for manual entry
    row_data['RetestPassCount'] = 0  # MOCK
    row_data['RetestFailCount'] = 0  # MOCK
    
    df_mapped = pd.concat([df_mapped, pd.DataFrame([row_data])], ignore_index=True)

print(f"   âœ… {len(df_mapped)} Ø±Ø¯ÛŒÙ Ù†Ú¯Ø§Ø´Øª Ø´Ø¯")

# ============================================================================
# STEP 5: Write to Excel
# ============================================================================

print("\nğŸ’¾ STEP 4: Ù†ÙˆØ´ØªÙ† Ø¨Ù‡ Excel...")

# Ensure column order matches original headers
df_mapped = df_mapped[original_headers]

# Write data to worksheet (starting from row 2)
for row_idx, row_data in df_mapped.iterrows():
    excel_row = row_idx + 2  # +2 because Excel is 1-indexed and row 1 is header
    
    for col_idx, header in enumerate(original_headers, 1):
        value = row_data[header]
        
        # Handle datetime
        if pd.notna(value) and isinstance(value, (pd.Timestamp, datetime)):
            value = value.to_pydatetime() if isinstance(value, pd.Timestamp) else value
        elif pd.isna(value):
            value = None
        
        ws_raw.cell(excel_row, col_idx, value)

print(f"   âœ… {len(df_mapped)} Ø±Ø¯ÛŒÙ Ø¨Ù‡ Excel Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯")

# ============================================================================
# STEP 6: Apply formatting
# ============================================================================

print("\nğŸ¨ STEP 5: Ø§Ø¹Ù…Ø§Ù„ Ù‚Ø§Ù„Ø¨â€ŒØ¨Ù†Ø¯ÛŒ...")

# Color code headers
GREEN = PatternFill(start_color='D4EDDA', end_color='D4EDDA', fill_type='solid')
YELLOW = PatternFill(start_color='FFF3CD', end_color='FFF3CD', fill_type='solid')
ORANGE = PatternFill(start_color='FFE5CC', end_color='FFE5CC', fill_type='solid')
BLUE = PatternFill(start_color='CCE5FF', end_color='CCE5FF', fill_type='solid')

green_fields = [
    'BugID', 'Title', 'Description', 'Severity', 'Priority', 'State', 'Category',
    'Tags', 'TeamName', 'ProjectName', 'SprintName', 'AssigneeName', 'ResolverName',
    'ClosedDate', 'ResolvedDate', 'LastModifiedDate', 'DueDate', 'CloseReason', 'IsRegression'
]

yellow_fields = [
    'CreatedDate', 'AssignedDate', 'TriageDate', 'InProgressDate',
    'ReadyForRetestDate', 'VerifiedDate', 'DoneDate',
    'ReopenCount', 'FirstReopenDate', 'LastReopenDate',
    'StateTransitionCount', 'StateChangeCount', 'AssigneeChangeCount',
    'StateHistory', 'PreviousState', 'is_escaped', 'StartedDate'
]

orange_fields = [
    'AssigneeID', 'ResolverID', 'Comments', 'LeadTimeHrs', 'CycleTimeHrs',
    'AgeDays', 'TriageDurationHrs', 'ActiveDurationHrs', 'InProgressDurationHrs',
    'ReadyForRetestDurationHrs', 'ResponseTimeHrs', 'WaitTimeHrs', 'ActiveWorkTimeHrs',
    'IsDuplicate', 'FixAttempts'
]

blue_fields = [
    'FixEffortHrs', 'ModuleName', 'RootCause', 'Resolution', 'TestCaseID',
    'AnalysisEffortHrs', 'DevEffortHrs', 'TestEffortHrs', 'ReopenEffortHrs',
    'TotalEffortHrs', 'EstimatedEffortHrs', 'VerifierName', 'VerifierID',
    'ReporterName', 'ReporterID', 'DuplicateOfBugID', 'RetestPassCount',
    'RetestFailCount', 'ExternalTicketID', 'ProjectID', 'TeamID', 'ModuleID', 'SprintID'
]

header_font = Font(bold=True, size=11, color='000000')

for col_idx, header in enumerate(original_headers, 1):
    cell = ws_raw.cell(1, col_idx)
    cell.font = header_font
    
    if header in green_fields:
        cell.fill = GREEN
    elif header in yellow_fields:
        cell.fill = YELLOW
    elif header in orange_fields:
        cell.fill = ORANGE
    elif header in blue_fields:
        cell.fill = BLUE

print(f"   âœ… Ø±Ù†Ú¯â€ŒØ¨Ù†Ø¯ÛŒ Ù‡Ø¯Ø±Ù‡Ø§ Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯")

# ============================================================================
# STEP 7: Save
# ============================================================================

print("\nğŸ’¾ STEP 6: Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„...")

output_file = 'BugTracking_Complete_REBUILT.xlsx'
wb.save(output_file)

import os
size_kb = os.path.getsize(output_file) / 1024

print(f"   âœ… Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_file}")
print(f"   ğŸ“ Ø­Ø¬Ù…: {size_kb:.1f} KB")

print("\n" + "=" * 80)
print("âœ… Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø´Ø¯!")
print("=" * 80)
print(f"""
ğŸ“Š Ø®Ù„Ø§ØµÙ‡:
   - Ø³Ø§Ø®ØªØ§Ø±: 74 ÙÛŒÙ„Ø¯ (Ù‡Ù…Ø§Ù†Ù†Ø¯ ÙØ§ÛŒÙ„ Ø§ØµÙ„ÛŒ)
   - Ø¯Ø§Ø¯Ù‡: {len(df_mapped)} Ø¨Ø§Ú¯ Ø§Ø² CSV ÙˆØ§Ù‚Ø¹ÛŒ
   - Ú†Ø§Ø±Øªâ€ŒÙ‡Ø§: 43 Ú†Ø§Ø±Øª (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
   - ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§: Ù‡Ù…Ù‡ ÙØ±Ù…ÙˆÙ„â€ŒÙ‡Ø§ Ø­ÙØ¸ Ø´Ø¯Ù‡
   
ğŸŸ¢ Green (19): Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² CSV
ğŸŸ¡ Yellow (17): MOCK - Ù†ÛŒØ§Ø² Ø¨Ù‡ WorkItemRevisions
ğŸŸ  Orange (15): Ù…Ø­Ø§Ø³Ø¨Ù‡â€ŒØ´Ø¯Ù‡
ğŸ”µ Blue (23): ÙˆØ±ÙˆØ¯ÛŒ Ø¯Ø³ØªÛŒ ÛŒØ§ N/A
""")
print("=" * 80)
